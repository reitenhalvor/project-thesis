{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global functions for deep learning\n",
    "---\n",
    "This notebook contains functions that are used in the making of the deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=100 # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras.__version__ = 2.2.4\n",
      "tf.__version__ = 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "from matplotlib import rc\n",
    "\n",
    "import keras\n",
    "print(\"keras.__version__ =\", keras.__version__ )\n",
    "from keras import layers, regularizers, Input, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "print(\"tf.__version__ =\", tf.__version__)\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from functions import MAE, RMSE, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib params and plotting\n",
    "## use seaborn as this gives nicer plots than the standard \n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "sns.set_style('whitegrid', {'axes.grid': True, 'grid.linestyle': '--'})\n",
    "\n",
    "rc('figure', figsize=(15,6))\n",
    "rc('xtick', labelsize=12)\n",
    "rc('ytick', labelsize=12)\n",
    "rc('axes', labelsize=13, titlesize=14)\n",
    "rc('legend', fontsize=14, handlelength=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for plotting and visualisation\n",
    "The following functions are used to visualise and make plots of the deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, savepath=None):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss history of the model in the training phase. \n",
    "    \"\"\"\n",
    "    epochs = history.epoch\n",
    "\n",
    "    train_mae = history.history['loss']\n",
    "    val_mae = history.history['val_loss']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_mae, marker='o', markersize='3.0', label=r'Training loss', color=\"darkred\")\n",
    "    plt.plot(epochs, val_mae, marker='o', markersize='3.0', label=r'Validation loss', color=\"darkblue\")  \n",
    "    plt.xlabel(r'Epoch')\n",
    "    plt.ylabel(r'MAE')\n",
    "    plt.legend(frameon=True)\n",
    "    if savepath is not None: \n",
    "        plt.savefig(savepath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_predictions(model, x_data, y_data, nruns=50):\n",
    "    \"\"\"\n",
    "    Make n predictions with a specified trained model. \n",
    "    \"\"\"\n",
    "    x = np.expand_dims(x_data, axis=0)\n",
    "    y_true = y_data\n",
    "    \n",
    "    preds_matr = np.zeros(shape=(nruns,y_true.shape[0],y_true.shape[1])) # (nruns, predictions, num_targets)\n",
    "    err_matr = np.zeros(shape=(nruns, y_true.shape[1]))  # (nruns, num_targets) - one error per target per run\n",
    "\n",
    "    for run in range(nruns):\n",
    "        preds = model.predict(x)[0]\n",
    "        err = MAE(y_true, preds, vector=True)\n",
    "        \n",
    "        preds_matr[run] = preds\n",
    "        err_matr[run] = err\n",
    "    \n",
    "    pred_means = np.array([np.mean(preds_matr[:,:,i], axis=0) for i in range(y_data.shape[-1])]).T\n",
    "    pred_stds = np.array([np.std(preds_matr[:,:,i], axis=0) for i in range(y_data.shape[-1])]).T\n",
    "    \n",
    "    return preds_matr, err_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_predictions(preds_matr, y_true, time_vec, target_tags, \n",
    "                              start_idx=0, n_obs=200, plotCI=False, savepath=None):\n",
    "    \"\"\"\n",
    "    Plots different realisations of the model, alternatively a 95% CI interval. \n",
    "    :param preds_matr: 3D matrix - The \"predictions matrix\" as obtained by the function make_n_predictions.\n",
    "    :param y_true: 2D matrix - The true target values, matrix of all targets. \n",
    "    :param start_idx: Integer - The start index the plot should start at.\n",
    "    :param plotCI: Boolean - If a CI interval should be plotted. \n",
    "    \n",
    "    :return None: Will only show the obtained plot. \n",
    "    \"\"\"\n",
    "    pred_means = np.array([np.mean(preds_matr[:,:,i], axis=0) for i in range(y_true.shape[-1])]).T\n",
    "    pred_stds = np.array([np.std(preds_matr[:,:,i], axis=0) for i in range(y_true.shape[-1])]).T\n",
    "    nruns = preds_matr.shape[0]\n",
    "    \n",
    "    start_idx = start_idx if start_idx < len(y_true) else max(0,len(y_true)-n_obs) \n",
    "    end_idx = min(len(y_true),start_idx+n_obs)\n",
    "    \n",
    "    time = time_vec[start_idx:end_idx]\n",
    "    \n",
    "    for output in range(preds_matr.shape[-1]):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        if not plotCI: # then plot individual predictions \n",
    "            for run in range(nruns):\n",
    "                preds = preds_matr[run, start_idx:end_idx, output]\n",
    "                plt.plot_date(time, preds, alpha=0.3, color=\"gray\", markersize=0, linestyle=\"-\")\n",
    "        else: \n",
    "            z = 1.96 #95% CI\n",
    "            CI_low = np.subtract(pred_means,pred_stds*z)\n",
    "            CI_high = np.add(pred_means,pred_stds*z)\n",
    "            plt.fill_between(time,\n",
    "                             CI_low[start_idx:end_idx,output], CI_high[start_idx:end_idx,output], \n",
    "                             color=\"gray\", alpha=0.5, label=\"95% CI\")\n",
    "    \n",
    "        y_pred_mean = pred_means[start_idx:end_idx,output]\n",
    "        y_signal_true = y_true[start_idx:end_idx, output]\n",
    "        \n",
    "        plt.plot_date(time, y_pred_mean, color=\"darkblue\", linewidth=1.5, linestyle=\"-\", markersize=0, label=\"Mean prediction\")\n",
    "        plt.plot_date(time, y_signal_true, color=\"darkred\", linewidth=1.5, linestyle=\"-\", markersize=0, label=\"True\")\n",
    "        plt.ylabel(target_tags[output])\n",
    "        plt.legend(frameon=True)\n",
    "        if savepath is not None:\n",
    "            spl = savepath.split(\".\")\n",
    "            plt.savefig(spl[0] + \"_{0}\".format(output) + \".\" + spl[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation function\n",
    "This is a function that will evaluate an architecture of a neural network. It will train the model on the training data and return the necessary error metrics for the validation and training data. \n",
    "\n",
    "**The inputs to the function (in order):**\n",
    "* `model` : A keras.models.Model() object. The architecture of the model\n",
    "* `x_train` : The features of the training data\n",
    "* `y_train` : The targets of the training data\n",
    "* `x_valid` : The features of the validation data\n",
    "* `y_valid` : The targets of the validation data\n",
    "* `x_test` : The features of the test data\n",
    "* `y_test` : The targets of the test data\n",
    "* `name` : The name of the model\n",
    "* `model_folder` : The path to the folder of the model (root folder to save plots, models etc.)\n",
    "* `lookback` : The number of time steps to \"look back\" in the training data. Default=540. \n",
    "* `batch_size` : The number of observations in a single batch, obtained by the `batch_generator()`. Default=256. \n",
    "* `epochs` : The number of epochs to train. Default=30.\n",
    "* `loss` : The loss metric to minimise. Default=`mae` - Mean Absolute Error. \n",
    "* `optimizer` : The optimizer to use when fitting the models. Default=`Adam()`\n",
    "* `generator` : The batch generator. Default=`batch_generator()` defined in `main/functions.py`\n",
    "* `nruns` : The number of runs that the predictions will be averaged over. Default=40,\n",
    "* `makeplots` : Boolean the model should display plots or not. Default=True. \n",
    "\n",
    "**The outputs of the function:**\n",
    "* Dictionary consisting of \n",
    "    * the trained model\n",
    "    * training history \n",
    "    * validation error metrics\n",
    "    * test error metrics. \n",
    "\n",
    "**Description:**\n",
    "\n",
    "The evaluation function will train the model on the training data and keep track of the weights that gave the minimum error. After all training epochs it will load the weights registered on the best run. The model will then make `n` predict values for the validation and test data using the function `make_n_predictions`. The following parameters are then computed:\n",
    "* `mean_preds` : The mean predictions for all features for all observations. Shape=(n_obs,n_targets).\n",
    "* `mean_stds` : The standard deviation of the predictions for all features for all observations. Shape=(n_obs,n_targets)\n",
    "* `expected_std` : The expected standard deviation. It is the average of `mean_stds`, i.e. the average standard deviation across all predictions for all runs. \n",
    "* `expected_mean` : The expected mean. It is the average of `mean_preds`, i.e. the average mean across all predictions for all runs. \n",
    "* `maes` : The expected MAE (averaged over `n` runs) for each target. Shape=(n_features)\n",
    "* `maes_unstd` : The expected _unstandardized_ MAE (averaged over `n` runs). Shape=(n_features)\n",
    "* `avg_mae` : The average of `maes`, i.e. the mean of the MAEs for all targets. \n",
    "\n",
    "All these parameters are calculated both for the validation set and the testing set and then collected in separate dictionaries. The model will print the performance for the validation data and testing data, before returning the trained model, training history, validation error metrics and test error metrics in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train, x_valid, y_valid, x_test, y_test, \n",
    "                   name, means, stds, root_path, feature_tags, target_tags,\n",
    "                   optimizer, generator, lookback=540, batch_size=256, epochs=30, loss='mae', nruns=40, makeplots=True):\n",
    "    \"\"\"\n",
    "    Will train the model and output model performance on validation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model_folder = root_path + \"models/{0}/{1}/\".format(name,epochs)\n",
    "    if not os.path.exists(model_folder):\n",
    "        print(\"Creating directory\", model_folder)\n",
    "        os.makedirs(model_folder)\n",
    "    \n",
    "    path_checkpoint = model_folder + \"weights.keras\".format(name)\n",
    "    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                          monitor='val_loss',\n",
    "                                          verbose=1,\n",
    "                                          save_weights_only=True,\n",
    "                                          save_best_only=True)\n",
    "    \n",
    "    '''\n",
    "    callback_reducelr = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                          factor = 0.5,\n",
    "                                          min_lr=0.001,\n",
    "                                          patience = 3, \n",
    "                                          verbose = 0)\n",
    "    \n",
    "    #callbacks = [callback_checkpoint, callback_reducelr]\n",
    "    '''\n",
    "    callbacks = [callback_checkpoint]\n",
    "    \n",
    "    train_gen = generator(x_train, y_train, lookback, batch_size)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    # train the model\n",
    "    train_steps = int(x_train.shape[0] // batch_size)\n",
    "    validation_data = (np.expand_dims(x_valid, axis=0),\n",
    "                       np.expand_dims(y_valid, axis=0))\n",
    "    \n",
    "    history = model.fit_generator(generator=train_gen,\n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch=train_steps,\n",
    "                                  validation_data=validation_data,\n",
    "                                  callbacks=callbacks)\n",
    "    \n",
    "    # save the model \n",
    "    model.save(model_folder + \"model.h5\".format(name))\n",
    "    \n",
    "    # plot history of training and validation loss\n",
    "    if makeplots:\n",
    "        plot_history(history, model_folder + \"history.png\".format(name))\n",
    "    \n",
    "    # load the weights that gave the best validation error\n",
    "    model.load_weights(path_checkpoint)\n",
    "    \n",
    "    # define some properties that we'll need\n",
    "    target_stds = stds[x_train.shape[1]:]\n",
    "    num_features = y_train.shape[-1]\n",
    "    \n",
    "    \n",
    "    # == VALIDATION DATA == #\n",
    "    ## Get the respecitve MAEs for each output\n",
    "    preds_val, errs_val = make_n_predictions(model, x_valid, y_valid, nruns)\n",
    "    mean_preds_val = np.array([np.mean(preds_val[:,:,i], axis=0) for i in range(num_features)]).T\n",
    "    stds_preds_val = np.array([np.std(preds_val[:,:,i], axis=0) for i in range(num_features)]).T \n",
    "    expected_std_val = np.mean(stds_preds_val,axis=0)   # the expected standard deviation of the predictions\n",
    "    expected_mean_val = np.mean(mean_preds_val,axis=0)  # the expected mean of the predictions\n",
    "\n",
    "    mae_val = np.mean(errs_val,axis=0)    # the mean standardized validation error for each target\n",
    "    mae_val_unstd = mae_val*target_stds   # the mean unstandardized validation error for each target\n",
    "    avg_mae_val = np.mean(mae_val)        # the single average of all mean validation errors\n",
    "    avg_mae_val_unstd = np.mean(mae_val_unstd)\n",
    "\n",
    "    ## collect in a dataframe\n",
    "    columns = [\"Tag\", \"MAE (std)\", \"MAE (unstd)\", \"Expect. Mean\", \"Expect. Stdev\"]\n",
    "    df_val = pd.DataFrame(np.column_stack([target_tags, mae_val, mae_val_unstd, expected_mean_val, expected_std_val]), \n",
    "                          columns=columns)\n",
    "    df_val.loc[len(df_val)] = ['Average', avg_mae_val, avg_mae_val_unstd, np.mean(expected_mean_val), np.mean(expected_std_val)]\n",
    "    str_table_val = tabulate(df_val, headers='keys', tablefmt='psql', floatfmt='.5f')\n",
    "\n",
    "    dict_val = {\n",
    "        'df':df_val,\n",
    "        'str_table':str_table_val,\n",
    "        'predictions_matrix': preds_val,\n",
    "        'predictions_mean': mean_preds_val,\n",
    "        'predictions_stds': stds_preds_val,\n",
    "    }\n",
    "    print(\"\\n       Validation data\")\n",
    "    print(str_table_val)\n",
    "    \n",
    "    \n",
    "    # == TEST DATA == #\n",
    "    ## Get the respecitve MAEs for each output\n",
    "    preds_test, errs_test = make_n_predictions(model, x_test, y_test, nruns)\n",
    "    mean_preds_test = np.array([np.mean(preds_test[:,:,i], axis=0) for i in range(num_features)]).T\n",
    "    stds_preds_test = np.array([np.std(preds_test[:,:,i], axis=0) for i in range(num_features)]).T \n",
    "    expected_std_test = np.mean(stds_preds_test,axis=0)   # the expected standard deviation of the predictions\n",
    "    expected_mean_test = np.mean(mean_preds_test,axis=0)  # the expected mean of the predictions\n",
    "\n",
    "    mae_test = np.mean(errs_test,axis=0)    # the mean standardized validation error for each target\n",
    "    mae_test_unstd = mae_test*target_stds   # the mean unstandardized validation error for each target\n",
    "    avg_mae_test = np.mean(mae_test)         # the single average of all mean validation errors\n",
    "    avg_mae_test_unstd = np.mean(mae_test_unstd)\n",
    "\n",
    "    ## collect in a dataframe\n",
    "    columns = [\"Tag\", \"MAE (std)\", \"MAE (unstd)\", \"Expect. Mean\", \"Expect. Stdev\"]\n",
    "    df_test = pd.DataFrame(np.column_stack([target_tags, mae_test, mae_test_unstd, expected_mean_test, expected_std_test]), \n",
    "                          columns=columns)\n",
    "    df_test.loc[len(df_test)] = ['Average', avg_mae_test, avg_mae_test_unstd, np.mean(expected_mean_test), np.mean(expected_std_test)]\n",
    "    str_table_test = tabulate(df_test, headers='keys', tablefmt='psql', floatfmt='.5f')\n",
    "    \n",
    "    dict_test = {\n",
    "        'df':df_test,\n",
    "        'str_table':str_table_test,\n",
    "        'predictions_matrix': preds_test,\n",
    "        'predictions_mean': mean_preds_test,\n",
    "        'predictions_stds': stds_preds_test,\n",
    "    }\n",
    "    print(\"\\n       Testing data\")\n",
    "    print(str_table_test)\n",
    "    \n",
    "    # Save files\n",
    "    np.save(model_folder + \"targets_valid.npy\", y_valid)\n",
    "    np.save(model_folder + \"targets_test.npy\", y_test)\n",
    "    np.save(model_folder + \"predictions_valid.npy\", preds_val)\n",
    "    np.save(model_folder + \"predictions_test.npy\", preds_test)\n",
    "    df_val.to_pickle(model_folder + \"dfval.pkl\")\n",
    "    df_test.to_pickle(model_folder + \"dftest.pkl\")\n",
    "    np.save(model_folder + \"table_valid.npy\", str_table_val)\n",
    "    np.save(model_folder + \"table_test.npy\", str_table_test)\n",
    "\n",
    "    # If makeplots = True, then plot the predictions with the mean \n",
    "    if makeplots: \n",
    "        plot_multiple_predictions(preds_val, y_valid, plotCI=True, \n",
    "                                  savepath=model_folder+\"{0}_uncertainty_plots.png\".format(name))\n",
    "    \n",
    "    return_dict = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'validation': dict_val,\n",
    "        'test': dict_test,\n",
    "    }\n",
    "    \n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary_for_dict(model_dict, y_valid, time_valid, y_test, time_test, target_tags, start_idx=2000, n_obs=6*60):\n",
    "    model_dict['model'].summary()\n",
    "\n",
    "    print(\"\\n      Validation data\")\n",
    "    print(model_dict['validation']['str_table'])\n",
    "\n",
    "    print(\"\\n      Testing data\")\n",
    "    print(model_dict['test']['str_table'])\n",
    "    \n",
    "    print(\"\\n Some sample plots:\")\n",
    "    print(\"---- Training vs validation history ----\")\n",
    "    plot_history(model_dict['history'])\n",
    "    \n",
    "    print(\"---- Plots of the predictions vs true ----\")\n",
    "    print(\"Validation data\")\n",
    "    plot_multiple_predictions(model_dict['validation']['predictions_matrix'],\n",
    "                             y_valid, time_valid, target_tags, start_idx,n_obs, plotCI=False)\n",
    "    \n",
    "    print(\"Test data\")\n",
    "    plot_multiple_predictions(model_dict['test']['predictions_matrix'],\n",
    "                         y_test, time_test, target_tags, start_idx,n_obs, plotCI=False)\n",
    "    \n",
    "    print(\"--- Plots with CI intervals ---\")\n",
    "    print(\"Validation data\")\n",
    "    plot_multiple_predictions(model_dict['validation']['predictions_matrix'],\n",
    "                             y_valid, time_valid, target_tags, start_idx,n_obs, plotCI=True)\n",
    "    \n",
    "    print(\"Test data\")\n",
    "    plot_multiple_predictions(model_dict['test']['predictions_matrix'],\n",
    "                         y_test, time_test, target_tags, start_idx,n_obs, plotCI=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_dicts(dicts, columns, index, texpath=None, round_digits=4):\n",
    "    \"\"\"\n",
    "    Will make a dataframe with error metrics for each target tag out of a collection of dictionaries \n",
    "    as obtained by evaluate_model(). The model names will be collected as indexes in the dataframe, and the \n",
    "    target errors in the columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    val_maes = []\n",
    "    test_maes = []\n",
    "    for d in dicts:\n",
    "        tmp_mae_val = [round(float(digit),round_digits) for digit in d['validation']['df']['MAE (std)'].tolist()]\n",
    "        tmp_mae_test = [round(float(digit),round_digits) for digit in d['test']['df']['MAE (std)'].tolist()]\n",
    "\n",
    "        val_maes.append(tmp_mae_val)\n",
    "        test_maes.append(tmp_mae_test)\n",
    "\n",
    "    # make df\n",
    "    df_val = pd.DataFrame(np.vstack(val_maes), index=index, columns=columns)\n",
    "    df_test = pd.DataFrame(np.vstack(test_maes), index=index, columns=columns)\n",
    "    df_summary = pd.concat([df_val, df_test], axis=1, keys=[\"Validation\", \"Test\"])\n",
    "\n",
    "    tex = df_summary.to_latex(column_format=\"l\" + \"c\"*(len(columns)*2),\n",
    "                              multicolumn=True, \n",
    "                              multicolumn_format='c', \n",
    "                              bold_rows=True)\n",
    "    if texpath is not None: \n",
    "        with open(texpath) as f:\n",
    "            f.write(tex)\n",
    "\n",
    "    return df_summary, tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_df_from_dicts(dicts, columns, index, levels, texpath=None, round_digits=4):\n",
    "    \"\"\"\n",
    "    Will make a dataframe with uncertainty and error metrics for each target tag out of a collection of dictionaries \n",
    "    as obtained by evaluate_model().\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframes = []\n",
    "    for d in dicts:\n",
    "        df = d['validation']\n",
    "        avg_maes = [round(float(digit),round_digits) for digit in df['df']['MAE (std)'].tolist()]\n",
    "        exp_means = [round(float(digit),round_digits) for digit in df['df']['Expect. Mean'].tolist()]\n",
    "        exp_stds = [round(float(digit),round_digits) for digit in df['df']['Expect. Stdev'].tolist()]\n",
    "        df_1 = pd.DataFrame(np.column_stack([avg_maes, exp_means, exp_stds]), index = levels, columns = columns)\n",
    "\n",
    "        df = d['test']\n",
    "        avg_maes = [round(float(digit),round_digits) for digit in df['df']['MAE (std)'].tolist()]\n",
    "        exp_means = [round(float(digit),round_digits) for digit in df['df']['Expect. Mean'].tolist()]\n",
    "        exp_stds = [round(float(digit),round_digits) for digit in df['df']['Expect. Stdev'].tolist()]\n",
    "        df_2 = pd.DataFrame(np.column_stack([avg_maes, exp_means, exp_stds]), index = levels, columns = columns)\n",
    "        df_concat = pd.concat([df_1, df_2], axis=1, keys=[\"Validation\", \"Test\"])\n",
    "        dataframes.append(df_concat)\n",
    "    \n",
    "    summary_df = pd.concat(dataframes, axis=0, keys=index)\n",
    "    \n",
    "    tex = summary_df.to_latex(column_format=\"ll\" + \"c\"*(len(columns)*2),\n",
    "                              multicolumn=True, \n",
    "                              multicolumn_format='c',\n",
    "                              multirow=True,\n",
    "                              bold_rows=True)\n",
    "\n",
    "    if texpath is not None: # save the file\n",
    "        with open(texpath, 'w+') as f:\n",
    "            f.write(tex)\n",
    "    \n",
    "    return summary_df, tex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
